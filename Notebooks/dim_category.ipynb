{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc24f119-22f6-489a-b605-86a9c10c5e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dimension Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d615ec-342d-4af4-8856-6a83bcea1827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>10100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select count(*) from parquet.`abfss://silver@contosoprojectstorage.dfs.core.windows.net/contoso_sales`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792ed1d2-f184-424d-9519-1518351dfa67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implementing SCD Type 2 (Add New Row): \n",
    "**Maintains full history by adding new rows for changes, using start/end dates and active flags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adcbd8c-d6c3-46c0-80da-879c51fd66a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category_id</th><th>category_key</th><th>category_name</th><th>is_current</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>Audio</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>2</td><td>2</td><td>TV and Video</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>4</td><td>4</td><td>Cameras and camcorders </td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>5</td><td>5</td><td>Cell phones</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>6</td><td>6</td><td>Music, Movies and Audio Books</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>7</td><td>7</td><td>Games and Toys</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>8</td><td>8</td><td>Home Appliances</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>10</td><td>3</td><td>NEW_Computers</td><td>1</td><td>2025-01-16</td><td>null</td></tr><tr><td>3</td><td>3</td><td>Computers</td><td>0</td><td>2025-01-16</td><td>2025-01-15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1,
         "Audio",
         1,
         "2025-01-16",
         null
        ],
        [
         2,
         2,
         "TV and Video",
         1,
         "2025-01-16",
         null
        ],
        [
         4,
         4,
         "Cameras and camcorders ",
         1,
         "2025-01-16",
         null
        ],
        [
         5,
         5,
         "Cell phones",
         1,
         "2025-01-16",
         null
        ],
        [
         6,
         6,
         "Music, Movies and Audio Books",
         1,
         "2025-01-16",
         null
        ],
        [
         7,
         7,
         "Games and Toys",
         1,
         "2025-01-16",
         null
        ],
        [
         8,
         8,
         "Home Appliances",
         1,
         "2025-01-16",
         null
        ],
        [
         10,
         3,
         "NEW_Computers",
         1,
         "2025-01-16",
         null
        ],
        [
         3,
         3,
         "Computers",
         0,
         "2025-01-16",
         "2025-01-15"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "category_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "category_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_current",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Get source data\n",
    "df_src = spark.sql('''\n",
    "    SELECT DISTINCT category_key, category_name\n",
    "    FROM parquet.`abfss://silver@contosoprojectstorage.dfs.core.windows.net/contoso_sales`\n",
    "''')\n",
    "\n",
    "# Initialize target DataFrame\n",
    "if spark.catalog.tableExists('contoso_catalog.gold.dim_category'):\n",
    "    df_tgt = spark.sql('''\n",
    "        SELECT  category_id, \n",
    "               category_key, \n",
    "               category_name,\n",
    "                is_current,\n",
    "                start_date,\n",
    "                end_date\n",
    "        FROM contoso_catalog.gold.dim_category\n",
    "    ''')\n",
    "else:\n",
    "    df_tgt = df_src.withColumn(\"category_id\", F.lit(0).cast(LongType()))\\\n",
    "                   .withColumn(\"is_current\", F.lit(1).cast(IntegerType()))\\\n",
    "                   .withColumn(\"start_date\", F.current_date())\\\n",
    "                   .withColumn(\"end_date\", F.lit(None).cast(DateType()))\\\n",
    "                   .filter(\"1 = 0\")  # Empty Schema\n",
    "\n",
    "def generate_surrogate_key(df, start_value):\n",
    "    w = Window.orderBy(\"category_key\")\n",
    "    return df.withColumn(\"category_id\", F.row_number().over(w) + F.lit(start_value))\n",
    "\n",
    "def apply_scd_type2_changes(df_src, df_tgt):\n",
    "    # Define the final column schema for consistency\n",
    "    final_columns = [\n",
    "        \"category_id\", \n",
    "        \"category_key\", \n",
    "        \"category_name\",\n",
    "        \"is_current\",\n",
    "        \"start_date\",\n",
    "        \"end_date\"\n",
    "    ]\n",
    "\n",
    "    # Step 1: Handle empty target\n",
    "    if df_tgt.rdd.isEmpty():\n",
    "        print(\"Target DataFrame is empty. Initializing with source records.\")\n",
    "        initial_df = df_src.withColumn(\"category_id\", F.lit(0).cast(LongType()))\\\n",
    "                          .withColumn(\"start_date\", F.current_date())\\\n",
    "                          .withColumn(\"end_date\", F.lit(None).cast(DateType()))\\\n",
    "                          .withColumn(\"is_current\", F.lit(1).cast(IntegerType()))\n",
    "        return generate_surrogate_key(initial_df, 0).select(final_columns)\n",
    "\n",
    "    # Step 2: Get max surrogate key\n",
    "    max_surrogate_key = df_tgt.agg(F.max(\"category_id\")).collect()[0][0] or 0\n",
    "    \n",
    "    # Step 3: Join source and target\n",
    "    src = df_src.alias(\"src\")\n",
    "    tgt = df_tgt.filter(F.col(\"is_current\") == 1).alias(\"tgt\")\n",
    "    joined_df = src.join(tgt, \"category_key\", \"outer\")\n",
    "\n",
    "    # Step 4: Identify new records\n",
    "    new_records = joined_df.filter(F.col(\"tgt.category_id\").isNull())\\\n",
    "                          .select(\"src.*\")\n",
    "    new_records_count = new_records.count()\n",
    "\n",
    "    if new_records_count > 0:\n",
    "        new_records = generate_surrogate_key(new_records, max_surrogate_key + 1)\\\n",
    "                     .withColumn(\"start_date\", F.current_date())\\\n",
    "                     .withColumn(\"end_date\", F.lit(None).cast(DateType()))\\\n",
    "                     .withColumn(\"is_current\", F.lit(1).cast(IntegerType()))\\\n",
    "                     .select(final_columns)\n",
    "    else:\n",
    "        new_records = spark.createDataFrame([], df_tgt.schema)\n",
    "\n",
    "    # Step 5: Identify changed records\n",
    "    changed_records = joined_df.filter(\n",
    "        (F.col(\"tgt.category_id\").isNotNull()) &\n",
    "        (F.coalesce(F.col(\"src.category_name\") != F.col(\"tgt.category_name\"), F.lit(False)))\n",
    "    )\n",
    "    changed_records_count = changed_records.count()\n",
    "\n",
    "    if changed_records_count > 0:\n",
    "        # Step 5.1: Create new versions for changed records\n",
    "        new_versions = changed_records.select(\"src.*\")\\\n",
    "                                    .withColumn(\"start_date\", F.current_date())\\\n",
    "                                    .withColumn(\"end_date\", F.lit(None).cast(DateType()))\\\n",
    "                                    .withColumn(\"is_current\", F.lit(1).cast(IntegerType()))\n",
    "        start_key = max_surrogate_key + new_records_count + 1\n",
    "        new_versions = generate_surrogate_key(new_versions, start_key).select(final_columns)\n",
    "\n",
    "        # Step 5.2: Close old versions\n",
    "        old_versions = df_tgt.join(\n",
    "            changed_records.select(\"category_key\"), \"category_key\", \"inner\"\n",
    "        ).withColumn(\"end_date\", F.when(\n",
    "            F.col(\"is_current\") == 1, F.date_sub(F.current_date(), 1)\n",
    "        ).otherwise(F.col(\"end_date\")))\\\n",
    "         .withColumn(\"is_current\", F.lit(0))\\\n",
    "         .select(final_columns)\n",
    "    else:\n",
    "        new_versions = spark.createDataFrame([], df_tgt.schema)\n",
    "        old_versions = spark.createDataFrame([], df_tgt.schema)\n",
    "\n",
    "    # Step 6: Unchanged records\n",
    "    unchanged_records = df_tgt.join(\n",
    "        changed_records.select(\"category_key\"), \"category_key\", \"leftanti\"\n",
    "    ).select(final_columns)\n",
    "\n",
    "    # Step 7: Combine results\n",
    "    final_df = unchanged_records.union(new_versions).union(old_versions).union(new_records)\n",
    "\n",
    "    return final_df.select(final_columns)\n",
    "\n",
    "# Apply SCD Type 2 changes\n",
    "result_df = apply_scd_type2_changes(df_src, df_tgt)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5ab06f-97ba-4341-b75c-ef9852639563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write results to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fde88b1-426a-4254-a6c1-a6b85d36ae35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result_df.write.format('delta') \\\n",
    "                  .mode('overwrite') \\\n",
    "                  .option('path', 'abfss://gold@contosoprojectstorage.dfs.core.windows.net/dim_category') \\\n",
    "                  .saveAsTable('contoso_catalog.gold.dim_category')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 687193511042160,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dim_category",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}