{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb92f1d-74b0-4383-ad2d-3dccedb30a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844276e6-82ca-41ba-ab84-d7bd9555e051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "            .option('header',True)\\\n",
    "                .option('inferSchema',True)\\\n",
    "                    .load('abfss://bronze@contosoprojectstorage.dfs.core.windows.net/Raw_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30d7fc9-fdb1-4719-b838-6eac36983aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from parquet.`abfss://bronze@contosoprojectstorage.dfs.core.windows.net/Raw_data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467698f4-1540-484f-8a3d-5bceddc48f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fixing column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a97f2d-4fa1-404a-a87e-d59ed2d86baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_to_snake_case(df):\n",
    "    \"\"\"\n",
    "    Convert column names from PascalCase or camelCase to snake_case in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with columns to be renamed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with column names converted to snake_case.\n",
    "    \"\"\"\n",
    "    # Get the list of column names\n",
    "    column_names = df.columns\n",
    "\n",
    "    # Dictionary to hold old and new column name mappings\n",
    "    rename_map = {}\n",
    "\n",
    "    for old_col_name in column_names:\n",
    "        # Convert column name from PascalCase or camelCase to snake_case\n",
    "        new_col_name = \"\".join([\n",
    "            \"_\" + char.lower() if (\n",
    "                char.isupper()              # Check if the current character is uppercase\n",
    "                and idx > 0                 # Ensure it's not the first character\n",
    "                and not old_col_name[idx - 1].isupper()  # Ensure the previous character is not uppercase\n",
    "            ) else char.lower()  # Convert character to lowercase\n",
    "            for idx, char in enumerate(old_col_name)\n",
    "        ]).lstrip(\"_\")  # Remove any leading underscore\n",
    "\n",
    "        # Avoid renaming to an existing column name\n",
    "        if new_col_name in rename_map.values():\n",
    "            raise ValueError(f\"Duplicate column name found after renaming: '{new_col_name}'\")\n",
    "\n",
    "        # Map the old column name to the new column name\n",
    "        rename_map[old_col_name] = new_col_name\n",
    "\n",
    "    # Rename columns using the mapping\n",
    "    for old_col_name, new_col_name in rename_map.items():\n",
    "        df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = rename_columns_to_snake_case(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a0c183-ac27-4f71-8a27-e304c98a51c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ffe0f8-141f-4e07-b928-b27a1088023a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "      .option('path','abfss://silver@contosoprojectstorage.dfs.core.windows.net/contoso_sales')\\\n",
    "        .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3855413826639767,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}